{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6426cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import heapq\n",
    "import itertools\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d1c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'data/merged/test/'\n",
    "train_dir = 'data/merged/train/'\n",
    "\n",
    "classes = sorted([d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))])\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dc0934-8a23-4c76-a883-9b27a75b2815",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'Number of classes: {num_classes}',\n",
    "    f'\\nClasses: {classes}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983c4b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------Train--------\")\n",
    "# check the number of images in each class\n",
    "for cls in classes:\n",
    "    print(f'{cls}: {len(os.listdir(train_dir + cls))}')\n",
    "\n",
    "print(\"\\n--------Test--------\")\n",
    "# check the number of images in each class\n",
    "for cls in classes:\n",
    "    print(f'{cls}: {len(os.listdir(test_dir + cls))}')\n",
    "    \n",
    "#print(\"\\n--------balance--------\")\n",
    "## check the number of images in each class\n",
    "#for cls in classes:\n",
    "#    print(f'{cls}: {len(os.listdir(b_train_dir + cls))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d54f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imghdr\n",
    "import shutil\n",
    "\n",
    "def delete_irrelevant_files(directory, classes):\n",
    "    for cls in classes:\n",
    "        class_dir = os.path.join(directory, cls)\n",
    "        with os.scandir(class_dir) as entries:\n",
    "            for entry in entries:\n",
    "                # Delete directories\n",
    "                if entry.is_dir():\n",
    "                    print(f'Deleting directory: {entry.path}')\n",
    "                    shutil.rmtree(entry.path)\n",
    "\n",
    "                # Check if the file is empty and delete it\n",
    "                elif os.path.getsize(entry.path) == 0:\n",
    "                    print(f'Deleting empty file: {entry.path}')\n",
    "                    os.remove(entry.path)\n",
    "\n",
    "                # Check if the file is not an image and delete it\n",
    "                elif not imghdr.what(entry.path):\n",
    "                    print(f'Deleting non-image file: {entry.path}')\n",
    "                    os.remove(entry.path)\n",
    "\n",
    "print(\"--------Train--------\")\n",
    "delete_irrelevant_files(train_dir, classes)\n",
    "\n",
    "print(\"\\n--------Test--------\")\n",
    "delete_irrelevant_files(test_dir, classes)\n",
    "\n",
    "#delete_irrelevant_files(b_train_dir, classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184cd348",
   "metadata": {},
   "source": [
    "# Landmark detection from images and correlation of those landmark positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d3d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = torch.Size([48, 48, 3])\n",
    "batch_size = 128\n",
    "\n",
    "# Compute the total size of the image input\n",
    "image_input_size = np.prod(image_size) * batch_size * 4. / (1024 ** 2.)\n",
    "print(f\"Image input size: {image_input_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc48fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FER_Dataset(Dataset):\n",
    "    def __init__(self, root_dir, classes, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # Keep a deterministic, sorted class order shared by train/val\n",
    "        self.classes = sorted(classes)\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "        self.samples = []\n",
    "        for label in self.classes:\n",
    "            class_dir = os.path.join(root_dir, label)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue\n",
    "            for img_name in sorted(os.listdir(class_dir)):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                # Lazy load; just store the path and label index\n",
    "                if os.path.isfile(img_path):\n",
    "                    self.samples.append((img_path, self.class_to_idx[label]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        trials = 0\n",
    "        while trials < 3:\n",
    "            img_path, label = self.samples[idx]\n",
    "            assert 0 <= label < len(self.classes), \"Label index out of range\"\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                trials += 1\n",
    "                idx = (idx + 1) % len(self.samples)\n",
    "                continue\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = Image.fromarray(image)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        raise RuntimeError(f\"Failed to read image after retries starting at index {idx}: {img_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde4c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/val transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.03, 0.03), scale=(0.95, 1.05), shear=3),\n",
    "    transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.03),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    transforms.RandomErasing(p=0.1, scale=(0.02, 0.08), ratio=(0.5, 2.0)),\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_dataset = FER_Dataset(train_dir, classes=classes, transform=train_transform)\n",
    "val_dataset = FER_Dataset(test_dir, classes=classes, transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e67fa6f-c829-4088-87a8-2a80c660a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders will be defined after class weights to attach the sampler.\n",
    "train_dataloader = None\n",
    "val_dataloader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b2c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_dataloader is not None and val_dataloader is not None:\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        print(i, data[0].shape, data[1].shape)\n",
    "        break\n",
    "    for i, data in enumerate(val_dataloader):\n",
    "        print(i, data[0].shape, data[1].shape)\n",
    "        break\n",
    "else:\n",
    "    print(\"Run the dataloader setup cell after class weights to initialize loaders before checking shapes.\")\n",
    "\n",
    "# torch.Size([256, 3, 48, 48]) torch.Size([4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d283a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = input_dim // num_heads\n",
    "\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "        self.fc = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value):\n",
    "        qk = torch.matmul(query, key.transpose(-2, -1))\n",
    "        dk = query.size(-1)\n",
    "        scaled_attention_logits = qk / torch.sqrt(torch.tensor(dk, dtype=torch.float32, device=qk.device))\n",
    "\n",
    "        attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        query = self.split_heads(query)\n",
    "        key = self.split_heads(key)\n",
    "        value = self.split_heads(value)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(query, key, value)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.input_dim)\n",
    "\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa64f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction=8, spatial_kernel=7):\n",
    "        super().__init__()\n",
    "        self.channel_mlp = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels, 1, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.spatial = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, kernel_size=spatial_kernel, padding=spatial_kernel // 2, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Channel attention\n",
    "        ca = self.channel_mlp(x)\n",
    "        x = x * ca\n",
    "        # Spatial attention\n",
    "        max_pool = torch.max(x, dim=1, keepdim=True)[0]\n",
    "        mean_pool = torch.mean(x, dim=1, keepdim=True)\n",
    "        sa = self.spatial(torch.cat([max_pool, mean_pool], dim=1))\n",
    "        return x * sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32162ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim * 4, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, Seq, Dim)\n",
    "        attn_out = self.attention(x)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = x + self.dropout(ff_out)\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "class HybridViTExpressionModel(nn.Module):\n",
    "    def __init__(self, num_classes, embed_dim=192, num_heads=4, depth=1, freeze_stages=3, dropout=0.5):\n",
    "        super().__init__()\n",
    "        weights = EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "        backbone = efficientnet_b0(weights=weights)\n",
    "        self.backbone = backbone.features\n",
    "        self.backbone_out_channels = 1280\n",
    "        \n",
    "        # Freeze early stages to retain pretrained low-level features\n",
    "        for idx, block in enumerate(self.backbone):\n",
    "            if idx < freeze_stages:\n",
    "                for param in block.parameters():\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        self.cbam = CBAM(self.backbone_out_channels)\n",
    "        self.proj = nn.Linear(self.backbone_out_channels, embed_dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "        \n",
    "        # Use custom TransformerBlock instead of nn.TransformerEncoderLayer for CoreML compatibility\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim, num_heads, dropout=dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        feats = self.cbam(feats)\n",
    "        b, c, h, w = feats.shape\n",
    "        tokens = feats.flatten(2).transpose(1, 2)  # (B, HW, C)\n",
    "        tokens = self.proj(tokens)\n",
    "        tokens = self.proj_drop(tokens)\n",
    "        tokens = self.transformer(tokens)\n",
    "        tokens = self.norm(tokens)\n",
    "        pooled = tokens.mean(dim=1)\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ba4b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mps_ok = torch.backends.mps.is_available()\n",
    "cuda_ok = torch.cuda.is_available()\n",
    "device = torch.device(\"mps\" if mps_ok else (\"cuda\" if cuda_ok else \"cpu\"))\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca52362",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "embed_dim = 192\n",
    "num_heads = 4\n",
    "transformer_depth = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab8a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure model exists before saving architecture snapshot\n",
    "if 'model' not in locals():\n",
    "    model = HybridViTExpressionModel(num_classes, embed_dim=embed_dim, num_heads=num_heads, depth=transformer_depth).to(device)\n",
    "\n",
    "with open('Architecture/HybridViTExpressionModel.txt', 'w') as f:\n",
    "    f.write(str(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a07a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, max_grad_norm=1.0, scaler=None, mixup_alpha=0.0, mixup_prob=0.0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0  # Use float to handle mixup weighted accuracy\n",
    "\n",
    "    # Only autocast on CUDA to avoid fp16 issues on MPS/CPU\n",
    "    use_autocast = torch.device(device).type == \"cuda\"\n",
    "    autocast_dtype = torch.float16 if use_autocast else torch.float32\n",
    "\n",
    "    for data, targets in dataloader:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Apply mixup with probability mixup_prob\n",
    "        apply_mixup = mixup_alpha > 0 and np.random.rand() < mixup_prob\n",
    "        if apply_mixup:\n",
    "            data, targets_a, targets_b, lam = mixup_data(data, targets, mixup_alpha)\n",
    "\n",
    "        if use_autocast:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=autocast_dtype):\n",
    "                outputs = model(data)\n",
    "                if apply_mixup:\n",
    "                    loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "                else:\n",
    "                    loss = criterion(outputs, targets)\n",
    "        else:\n",
    "            outputs = model(data)\n",
    "            if apply_mixup:\n",
    "                loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "            else:\n",
    "                loss = criterion(outputs, targets)\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        running_loss += loss.item() * data.size(0)\n",
    "        # For mixup, accuracy is approximate (weighted sum of matches)\n",
    "        if apply_mixup:\n",
    "            correct_a = (preds == targets_a).sum().item()\n",
    "            correct_b = (preds == targets_b).sum().item()\n",
    "            running_corrects += lam * correct_a + (1 - lam) * correct_b\n",
    "        else:\n",
    "            running_corrects += (preds == targets).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = running_corrects / len(dataloader.dataset)\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31561c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    use_autocast = torch.device(device).type == \"cuda\"\n",
    "    autocast_dtype = torch.float16 if use_autocast else torch.float32\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in dataloader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            if use_autocast:\n",
    "                with torch.amp.autocast(device_type=\"cuda\", dtype=autocast_dtype):\n",
    "                    outputs = model(data)\n",
    "                    loss = criterion(outputs, targets)\n",
    "            else:\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * data.size(0)\n",
    "            running_corrects += torch.sum(preds == targets.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = running_corrects.float() / float(len(dataloader.dataset))\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488203d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import WeightedRandomSampler \n",
    "\n",
    "# Class weighting for imbalance\n",
    "eps = 1e-6\n",
    "class_counts = torch.tensor([len(os.listdir(os.path.join(train_dir, cls))) for cls in classes], device=\"cpu\", dtype=torch.float)\n",
    "class_weights = (class_counts.sum() / (class_counts + eps)).float()\n",
    "class_weights = class_weights / class_weights.mean()\n",
    "\n",
    "# ---------- ANTI-OVERFITTING HYPERPARAMETERS ----------\n",
    "base_lr = 2e-4           # Slightly lower LR\n",
    "weight_decay = 5e-3      # Stronger L2 regularization\n",
    "warmup_epochs = 3        # Shorter warmup\n",
    "num_epochs = 60          # Fewer epochs (early stopping will kick in)\n",
    "dropout = 0.5            # Higher dropout (passed to model)\n",
    "mixup_alpha = 0.2        # Mixup augmentation strength\n",
    "mixup_prob = 0.5         # Probability of applying mixup per batch\n",
    "# -------------------------------------------------------\n",
    "\n",
    "model = HybridViTExpressionModel(num_classes, embed_dim=embed_dim, num_heads=num_heads, depth=transformer_depth, dropout=dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device), label_smoothing=0.1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "cosine_scheduler = CosineAnnealingLR(optimizer, T_max=max(num_epochs - warmup_epochs, 1), eta_min=1e-6)\n",
    "scaler = GradScaler(enabled=device.type == \"cuda\")\n",
    "\n",
    "# Mixup utility function\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"Apply mixup augmentation to a batch.\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.0\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size, device=x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Compute mixup loss.\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18244af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = torch.tensor([class_weights[label] for _, label in train_dataset.samples], dtype=torch.double)\n",
    "train_sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, shuffle=False, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828ea399",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d74a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import builtins\n",
    "\n",
    "# create empty lists to store loss and accuracy for each epoch\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "max_grad_norm = 1.0\n",
    "early_stop_patience = 15  # Tighter early stopping\n",
    "best_val_loss = float(\"inf\")\n",
    "no_improve_epochs = 0\n",
    "\n",
    "device_type = torch.device(device).type\n",
    "\n",
    "if not hasattr(builtins, \"_fer_training_started\"):\n",
    "    builtins._fer_training_started = True\n",
    "\n",
    "    run_id = torch.randint(0, 1_000_000, ()).item()\n",
    "    print(f\"Starting training run_id={run_id}\")\n",
    "    print(f\"Overfitting mitigations: dropout={dropout}, weight_decay={weight_decay}, mixup_alpha={mixup_alpha}, mixup_prob={mixup_prob}, epochs={num_epochs}, early_stop={early_stop_patience}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_dataloader, criterion, optimizer, device, \n",
    "            max_grad_norm=max_grad_norm, scaler=scaler,\n",
    "            mixup_alpha=mixup_alpha, mixup_prob=mixup_prob\n",
    "        )\n",
    "        val_loss, val_acc = validate_epoch(model, val_dataloader, criterion, device)\n",
    "        \n",
    "        # Warmup then cosine schedule\n",
    "        if epoch < warmup_epochs:\n",
    "            warmup_lr = base_lr * float(epoch + 1) / float(warmup_epochs)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = warmup_lr\n",
    "        else:\n",
    "            cosine_scheduler.step()\n",
    "        \n",
    "        # append the loss and accuracy to the lists\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_acc_list.append(train_acc)\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_acc_list.append(val_acc)\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"[run_id={run_id}] Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, LR: {current_lr:.6f}\")\n",
    "\n",
    "        # Early stopping on validation loss\n",
    "        if val_loss < best_val_loss - 1e-4:\n",
    "            best_val_loss = val_loss\n",
    "            no_improve_epochs = 0\n",
    "            # Save best model checkpoint\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            if no_improve_epochs >= early_stop_patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model after training\n",
    "    if os.path.exists(\"best_model.pth\"):\n",
    "        model.load_state_dict(torch.load(\"best_model.pth\", weights_only=True))\n",
    "        print(\"Loaded best model checkpoint.\")\n",
    "else:\n",
    "    print(\"Training already started in this kernel; restart kernel to run again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d739b73-3926-4de7-94eb-ec7cd6d2bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "dummy_input = torch.randn(1, 3, 128, 128).to(device)\n",
    "\n",
    "onnx_filename = \"FER_Model_Adam.onnx\"\n",
    "torch.onnx.export(model, dummy_input, onnx_filename)\n",
    "\n",
    "print(f\"Model saved as {onnx_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b359a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools as ct\n",
    "\n",
    "# Now that we use a custom TransformerBlock (standard ops), we can convert directly from PyTorch.\n",
    "model.eval()\n",
    "model_cpu = model.to(\"cpu\")\n",
    "example = torch.randn(1, 3, 128, 128)\n",
    "\n",
    "# Trace the model\n",
    "traced = torch.jit.trace(model_cpu, example)\n",
    "\n",
    "# Convert to CoreML\n",
    "mlmodel = ct.convert(\n",
    "    traced,\n",
    "    inputs=[ct.ImageType(name=\"image\", shape=example.shape, scale=1/127.5, bias=[-1.0, -1.0, -1.0], color_layout=\"RGB\")],\n",
    "    minimum_deployment_target=ct.target.iOS15,\n",
    "    convert_to=\"mlprogram\"\n",
    ")\n",
    "\n",
    "mlmodel.save(\"FER_Model.mlmodel\")\n",
    "model.to(device)\n",
    "print(\"Model saved as FER_Model.mlmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c3ba05-e6dc-4393-877b-a795bc38c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_acc_list = [float(t) for t in train_acc_list]\n",
    "val_acc_list = [float(t) for t in val_acc_list]\n",
    "\n",
    "train_acc_df = pd.DataFrame({'Epoch': range(len(train_acc_list)), 'Accuracy': train_acc_list, 'Type': 'Train'})\n",
    "val_acc_df = pd.DataFrame({'Epoch': range(len(val_acc_list)), 'Accuracy': val_acc_list, 'Type': 'Validation'})\n",
    "\n",
    "acc_df = pd.concat([train_acc_df, val_acc_df])\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=acc_df, x='Epoch', y='Accuracy', hue='Type')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Save the graph to a file\n",
    "plt.savefig('accuracy_graph_Adam.png', dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d842c20a-c235-46aa-944d-8f5be5a6419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = [float(t) for t in train_loss_list]\n",
    "val_loss_list = [float(t) for t in val_loss_list]\n",
    "\n",
    "train_loss_list = pd.DataFrame({'Epoch': range(len(train_loss_list)), 'Loss': train_loss_list, 'Type': 'Train'})\n",
    "val_loss_list = pd.DataFrame({'Epoch': range(len(val_loss_list)), 'Loss': val_loss_list, 'Type': 'Validation'})\n",
    "\n",
    "acc_df = pd.concat([train_loss_list, val_loss_list])\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=acc_df, x='Epoch', y='Loss', hue='Type')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Save the graph to a file\n",
    "plt.savefig('loss_graph_Adam.png', dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db09c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a function to check pytorch version\n",
    "def check_pytorch_version():\n",
    "    if torch.__version__ >= '1.6.0':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# define a function to check cuda version\n",
    "def check_cuda_version():\n",
    "    if torch.cuda.is_available():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# define a function to check cudnn version\n",
    "def check_cudnn_version():\n",
    "    if check_cuda_version():\n",
    "        if torch.backends.cudnn.enabled:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# define a function to check if the system is ready for training\n",
    "def check_system():\n",
    "    if check_pytorch_version():\n",
    "        print('PyTorch version: {}'.format(torch.__version__))\n",
    "    else:\n",
    "        print('PyTorch version: {} (update required)'.format(torch.__version__))\n",
    "        \n",
    "    if check_cuda_version():\n",
    "        print('CUDA version: {}'.format(torch.version.cuda))\n",
    "    else:\n",
    "        print('CUDA version: {} (install CUDA to enable GPU training)'.format(torch.version.cuda))\n",
    "        \n",
    "    if check_cudnn_version():\n",
    "        print('cuDNN version: {}'.format(torch.backends.cudnn.version()))\n",
    "    else:\n",
    "        print('cuDNN version: {} (install cuDNN to enable GPU training)'.format(torch.backends.cudnn.version()))\n",
    "\n",
    "check_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21299af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ec6f1b2",
   "metadata": {},
   "source": [
    "## Plan: MediaPipe Landmarks + Feature Fusion\n",
    "- Extract 468-point face mesh (x, y, z) with MediaPipe during data loading; cache to disk to avoid recompute.\n",
    "- Build geometric features: per-point normalized coords, eye/mouth aspect ratios, distances to nose tip, angles for eyebrows/mouth corners, and temporal deltas if using video.\n",
    "- Create a small MLP branch (e.g., Linear->GELU->Dropout) to embed landmark features, then concatenate with the CNN-Transformer pooled embedding before the final classifier.\n",
    "- Normalize landmarks per face (center to nose tip, scale by inter-ocular distance) to reduce variance across subjects.\n",
    "- For inference, run MediaPipe once per frame, reuse the same ring buffer to smooth both pixel and landmark branches, then fuse and classify.\n",
    "- Efficiency: MediaPipe on CPU is typically <3 ms on laptop CPUs; the fused model keeps the vision backbone unchanged, so latency overhead is modest while adding strong pose/shape cues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c_vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
