{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c67f5f6d",
   "metadata": {},
   "source": [
    "# Started from here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6426cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import heapq\n",
    "import itertools\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d71d1c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'data/merged/test/'\n",
    "train_dir = 'data/merged/train/'\n",
    "\n",
    "#b_train_dir = 'data/balanced/'\n",
    "\n",
    "classes = os.listdir(train_dir)\n",
    "test_cls = os.listdir(train_dir)\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7dc0934-8a23-4c76-a883-9b27a75b2815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 7 \n",
      "Classes: ['fear', 'angry', 'sad', 'neutral', 'surprise', 'disgust', 'happy']\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'Number of classes: {num_classes}',\n",
    "    f'\\nClasses: {classes}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "983c4b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Train--------\n",
      "fear: 5000\n",
      "angry: 5000\n",
      "sad: 5000\n",
      "neutral: 5000\n",
      "surprise: 5000\n",
      "disgust: 5000\n",
      "happy: 7215\n",
      "\n",
      "--------Test--------\n",
      "fear: 1024\n",
      "angry: 1000\n",
      "sad: 1247\n",
      "neutral: 1233\n",
      "surprise: 1000\n",
      "disgust: 1000\n",
      "happy: 1774\n"
     ]
    }
   ],
   "source": [
    "print(\"--------Train--------\")\n",
    "# check the number of images in each class\n",
    "for cls in classes:\n",
    "    print(f'{cls}: {len(os.listdir(train_dir + cls))}')\n",
    "\n",
    "print(\"\\n--------Test--------\")\n",
    "# check the number of images in each class\n",
    "for cls in classes:\n",
    "    print(f'{cls}: {len(os.listdir(test_dir + cls))}')\n",
    "    \n",
    "#print(\"\\n--------balance--------\")\n",
    "## check the number of images in each class\n",
    "#for cls in classes:\n",
    "#    print(f'{cls}: {len(os.listdir(b_train_dir + cls))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59d54f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Train--------\n",
      "\n",
      "--------Test--------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import imghdr\n",
    "import shutil\n",
    "\n",
    "def delete_irrelevant_files(directory, classes):\n",
    "    for cls in classes:\n",
    "        class_dir = os.path.join(directory, cls)\n",
    "        with os.scandir(class_dir) as entries:\n",
    "            for entry in entries:\n",
    "                # Delete directories\n",
    "                if entry.is_dir():\n",
    "                    print(f'Deleting directory: {entry.path}')\n",
    "                    shutil.rmtree(entry.path)\n",
    "\n",
    "                # Check if the file is empty and delete it\n",
    "                elif os.path.getsize(entry.path) == 0:\n",
    "                    print(f'Deleting empty file: {entry.path}')\n",
    "                    os.remove(entry.path)\n",
    "\n",
    "                # Check if the file is not an image and delete it\n",
    "                elif not imghdr.what(entry.path):\n",
    "                    print(f'Deleting non-image file: {entry.path}')\n",
    "                    os.remove(entry.path)\n",
    "\n",
    "print(\"--------Train--------\")\n",
    "delete_irrelevant_files(train_dir, classes)\n",
    "\n",
    "print(\"\\n--------Test--------\")\n",
    "delete_irrelevant_files(test_dir, classes)\n",
    "\n",
    "#delete_irrelevant_files(b_train_dir, classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184cd348",
   "metadata": {},
   "source": [
    "# Landmark detection from images and correlation of those landmark positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "687d3d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image input size: 6.75 MB\n"
     ]
    }
   ],
   "source": [
    "image_size = torch.Size([48, 48, 3])\n",
    "batch_size = 256\n",
    "\n",
    "# Compute the total size of the image input\n",
    "image_input_size = np.prod(image_size) * batch_size * 4. / (1024 ** 2.)\n",
    "print(f\"Image input size: {image_input_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc48fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FER_Dataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.labels = []  # new list to store the label index\n",
    "        for i, label in enumerate(os.listdir(root_dir)):\n",
    "            class_dir = os.path.join(root_dir, label)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                image = cv2.imread(img_path)\n",
    "                self.data.append(image)\n",
    "                self.labels.append(i)  # add the label index for this image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]  # get the label index for this image\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label  # return the image and label index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fde4c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = FER_Dataset(train_dir, transform=transform)\n",
    "val_dataset = FER_Dataset(test_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e67fa6f-c829-4088-87a8-2a80c660a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=24)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5b2c0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([256, 3, 128, 128]) torch.Size([256])\n",
      "0 torch.Size([256, 3, 128, 128]) torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# show shape of the train_dataloader\n",
    "for i, data in enumerate(train_dataloader):\n",
    "    print(i, data[0].shape, data[1].shape)\n",
    "    break\n",
    "\n",
    "# show shape of the val_dataloader\n",
    "for i, data in enumerate(val_dataloader):\n",
    "    print(i, data[0].shape, data[1].shape)\n",
    "    break\n",
    "\n",
    "# torch.Size([4, 3, 48, 48]) torch.Size([4])\n",
    "# torch.Size([4, 3, 48, 48]) torch.Size([4])\n",
    "\n",
    "# 0 torch.Size([256, 3, 64, 64]) torch.Size([256])\n",
    "# 0 torch.Size([256, 3, 64, 64]) torch.Size([256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d283a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = input_dim // num_heads\n",
    "\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "        self.fc = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value):\n",
    "        qk = torch.matmul(query, key.transpose(-2, -1))\n",
    "        dk = query.size(-1)\n",
    "        scaled_attention_logits = qk / torch.sqrt(torch.tensor(dk, dtype=torch.float32, device=qk.device))\n",
    "\n",
    "        attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        query = self.split_heads(query)\n",
    "        key = self.split_heads(key)\n",
    "        value = self.split_heads(value)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(query, key, value)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.input_dim)\n",
    "\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c963dd2c-7902-4408-8f9b-b2a83a2dd1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yolov5\n",
      "  Using cached yolov5-7.0.11-py37.py38.py39.py310-none-any.whl (956 kB)\n",
      "Requirement already satisfied: boto3>=1.19.1 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from yolov5) (1.26.60)\n",
      "Requirement already satisfied: ipython in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from yolov5) (8.6.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from yolov5) (1.23.4)\n",
      "Requirement already satisfied: requests>=2.23.0 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from yolov5) (2.28.1)\n",
      "Requirement already satisfied: psutil in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from yolov5) (5.9.4)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from yolov5) (1.5.3)\n",
      "Collecting huggingface-hub>=0.12.0\n",
      "  Using cached huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from yolov5) (9.4.0)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from yolov5) (0.12.2)\n",
      "Collecting sahi>=0.11.10\n",
      "  Downloading sahi-0.11.13-py3-none-any.whl (100 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.0/101.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.64.0 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from yolov5) (4.64.1)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from yolov5) (3.6.2)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from yolov5) (1.10.0)\n",
      "Requirement already satisfied: gitpython in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from yolov5) (3.1.31)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from yolov5) (0.14.0a0)\n",
      "Requirement already satisfied: torch>=1.7.0 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from yolov5) (1.13.0)\n",
      "Collecting roboflow>=0.2.29\n",
      "  Using cached roboflow-1.0.1-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from yolov5) (4.6.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from yolov5) (6.0)\n",
      "Requirement already satisfied: thop>=0.1.1 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from yolov5) (0.1.1.post2209072238)\n",
      "Collecting fire\n",
      "  Using cached fire-0.5.0.tar.gz (88 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tensorboard>=2.4.1 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from yolov5) (2.11.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from boto3>=1.19.1->yolov5) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from boto3>=1.19.1->yolov5) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.60 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from boto3>=1.19.1->yolov5) (1.29.60)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from huggingface-hub>=0.12.0->yolov5) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from huggingface-hub>=0.12.0->yolov5) (4.4.0)\n",
      "Requirement already satisfied: filelock in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from huggingface-hub>=0.12.0->yolov5) (3.8.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from matplotlib>=3.2.2->yolov5) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from matplotlib>=3.2.2->yolov5) (4.38.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from matplotlib>=3.2.2->yolov5) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from matplotlib>=3.2.2->yolov5) (1.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from matplotlib>=3.2.2->yolov5) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from matplotlib>=3.2.2->yolov5) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from pandas>=1.1.4->yolov5) (2022.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from requests>=2.23.0->yolov5) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from requests>=2.23.0->yolov5) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from requests>=2.23.0->yolov5) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from requests>=2.23.0->yolov5) (2.1.1)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: six in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from roboflow>=0.2.29->yolov5) (1.16.0)\n",
      "Requirement already satisfied: wget in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from roboflow>=0.2.29->yolov5) (3.2)\n",
      "Collecting requests-toolbelt\n",
      "  Using cached requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "Requirement already satisfied: python-dotenv in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from roboflow>=0.2.29->yolov5) (0.21.1)\n",
      "Collecting chardet==4.0.0\n",
      "  Using cached chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "Collecting pyparsing>=2.2.1\n",
      "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: shapely>=1.8.0 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from sahi>=0.11.10->yolov5) (2.0.1)\n",
      "Collecting pybboxes==0.1.6\n",
      "  Using cached pybboxes-0.1.6-py3-none-any.whl (24 kB)\n",
      "Collecting terminaltables\n",
      "  Using cached terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
      "Collecting click==8.0.4\n",
      "  Using cached click-8.0.4-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (1.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (0.6.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (0.38.4)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (1.51.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (65.5.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (2.15.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (2.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (3.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (0.4.6)\n",
      "Requirement already satisfied: protobuf in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (3.20.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (1.8.1)\n",
      "Requirement already satisfied: termcolor in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from fire->yolov5) (2.2.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from gitpython->yolov5) (4.0.10)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from ipython->yolov5) (0.18.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from ipython->yolov5) (4.8.0)\n",
      "Requirement already satisfied: backcall in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from ipython->yolov5) (0.2.0)\n",
      "Requirement already satisfied: stack-data in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from ipython->yolov5) (0.6.1)\n",
      "Requirement already satisfied: traitlets>=5 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from ipython->yolov5) (5.5.0)\n",
      "Requirement already satisfied: decorator in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from ipython->yolov5) (5.1.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from ipython->yolov5) (2.13.0)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from ipython->yolov5) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from ipython->yolov5) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from ipython->yolov5) (3.0.32)\n",
      "Requirement already satisfied: appnope in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from ipython->yolov5) (0.1.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython->yolov5) (5.0.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (0.2.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->yolov5) (1.3.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from jedi>=0.16->ipython->yolov5) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from pexpect>4.3->ipython->yolov5) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython->yolov5) (0.2.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->yolov5) (2.1.1)\n",
      "Requirement already satisfied: pure-eval in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from stack-data->ipython->yolov5) (0.2.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from stack-data->ipython->yolov5) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from stack-data->ipython->yolov5) (2.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->yolov5) (3.2.2)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116931 sha256=eda09413ff7fcc8338ccd4ad6cebbfd9ce6dddaaff65b5d4276ccbffb87628f8\n",
      "  Stored in directory: /Users/sertanavdan/Library/Caches/pip/wheels/c4/eb/6a/1c6d2ad660043768e998bdf9c6a28db2f1b7db3a5825d51e87\n",
      "Successfully built fire\n",
      "Installing collected packages: terminaltables, pyparsing, pybboxes, idna, fire, cycler, click, chardet, sahi, requests-toolbelt, huggingface-hub, roboflow, yolov5\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.0.9\n",
      "    Uninstalling pyparsing-3.0.9:\n",
      "      Successfully uninstalled pyparsing-3.0.9\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: cycler\n",
      "    Found existing installation: cycler 0.11.0\n",
      "    Uninstalling cycler-0.11.0:\n",
      "      Successfully uninstalled cycler-0.11.0\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.3\n",
      "    Uninstalling click-8.1.3:\n",
      "      Successfully uninstalled click-8.1.3\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 5.1.0\n",
      "    Uninstalling chardet-5.1.0:\n",
      "      Successfully uninstalled chardet-5.1.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.11.0\n",
      "    Uninstalling huggingface-hub-0.11.0:\n",
      "      Successfully uninstalled huggingface-hub-0.11.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "docker-compose 1.29.2 requires jsonschema<4,>=2.5.1, but you have jsonschema 4.17.3 which is incompatible.\n",
      "docker-compose 1.29.2 requires PyYAML<6,>=3.10, but you have pyyaml 6.0 which is incompatible.\n",
      "docker-compose 1.29.2 requires websocket-client<1,>=0.32.0, but you have websocket-client 1.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed chardet-4.0.0 click-8.0.4 cycler-0.10.0 fire-0.5.0 huggingface-hub-0.13.3 idna-2.10 pybboxes-0.1.6 pyparsing-2.4.7 requests-toolbelt-0.10.1 roboflow-1.0.1 sahi-0.11.13 terminaltables-3.1.10 yolov5-7.0.11\n"
     ]
    }
   ],
   "source": [
    "!pip install yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ed7e7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yolov5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/Shared/D/Applied AI Solutions/DL2/FER/FINAL/submit_/FER_Landmark_Attention_YOLOv5-V1 (1).ipynb Cell 15\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Shared/D/Applied%20AI%20Solutions/DL2/FER/FINAL/submit_/FER_Landmark_Attention_YOLOv5-V1%20%281%29.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39myolov5\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39myolo\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Shared/D/Applied%20AI%20Solutions/DL2/FER/FINAL/submit_/FER_Landmark_Attention_YOLOv5-V1%20%281%29.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_pretrained_yolov5\u001b[39m(path: \u001b[39mstr\u001b[39m, num_classes: \u001b[39mint\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Shared/D/Applied%20AI%20Solutions/DL2/FER/FINAL/submit_/FER_Landmark_Attention_YOLOv5-V1%20%281%29.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(path)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yolov5'"
     ]
    }
   ],
   "source": [
    "from yolov5.models.yolo import Model\n",
    "\n",
    "def load_pretrained_yolov5(path: str, num_classes: int):\n",
    "    checkpoint = torch.load(path)\n",
    "    model = Model(checkpoint['model'].yaml)\n",
    "    model.load_state_dict(checkpoint['model'].state_dict())\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        # Freeze all layers except the last three\n",
    "        if not any(name.startswith(f\"model.{i}\") for i in [17, 20, 23]):\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Replace the last layer with a new one, adjust the number of classes\n",
    "    model.model[-1].nc = num_classes  # Set the number of classes in the Detect layer\n",
    "\n",
    "    # Reinitialize the last layer's weights\n",
    "    for m in model.modules():\n",
    "        if type(m) is nn.Conv2d:\n",
    "            torch.nn.init.normal_(m.weight, std=0.01)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32162ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialExpressionDetectionModel(nn.Module):\n",
    "    def __init__(self, num_classes, num_heads):\n",
    "        super(FacialExpressionDetectionModel, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # Function to load the pretrained YOLOv5 model\n",
    "        self.yolov5 = load_pretrained_yolov5(path=\"yolov5s.pt\",num_classes=num_classes)\n",
    "        self.yolov5 = nn.Sequential(*list(self.yolov5.children())[:-1])  # Remove the Detect layer\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        # Block-2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 512, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        # Block-3\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "\n",
    "        # Block-4\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.125)\n",
    "        )\n",
    "\n",
    "        # Block-5\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(256 * 8 * 8, 512),  # Update the input size to match the new input shape\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "\n",
    "        # Attention Layer\n",
    "        self.attention = MultiHeadAttention(512, num_heads)\n",
    "        self.attention_transform = nn.Linear(512, 64)\n",
    "\n",
    "        # Block-6\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(64, 32),  # Update the input size to 64\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.125),\n",
    "        )\n",
    "\n",
    "        # Block-7\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.yolov5(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.attention(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output of the attention layer\n",
    "        x = self.attention_transform(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return F.softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0ba4b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aca52362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  yolov5.models.common.Conv               [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  yolov5.models.common.Conv               [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  yolov5.models.common.C3                 [64, 64, 1]                   \n",
      "  3                -1  1     73984  yolov5.models.common.Conv               [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  yolov5.models.common.C3                 [128, 128, 2]                 \n",
      "  5                -1  1    295424  yolov5.models.common.Conv               [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  yolov5.models.common.C3                 [256, 256, 3]                 \n",
      "  7                -1  1   1180672  yolov5.models.common.Conv               [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  yolov5.models.common.C3                 [512, 512, 1]                 \n",
      "  9                -1  1    656896  yolov5.models.common.SPPF               [512, 512, 5]                 \n",
      " 10                -1  1    131584  yolov5.models.common.Conv               [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  yolov5.models.common.Concat             [1]                           \n",
      " 13                -1  1    361984  yolov5.models.common.C3                 [512, 256, 1, False]          \n",
      " 14                -1  1     33024  yolov5.models.common.Conv               [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  yolov5.models.common.Concat             [1]                           \n",
      " 17                -1  1     90880  yolov5.models.common.C3                 [256, 128, 1, False]          \n",
      " 18                -1  1    147712  yolov5.models.common.Conv               [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  yolov5.models.common.Concat             [1]                           \n",
      " 20                -1  1    296448  yolov5.models.common.C3                 [256, 256, 1, False]          \n",
      " 21                -1  1    590336  yolov5.models.common.Conv               [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  yolov5.models.common.Concat             [1]                           \n",
      " 23                -1  1   1182720  yolov5.models.common.C3                 [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  yolov5.models.yolo.Detect               [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_classes = 7\n",
    "num_heads = 32\n",
    "model = FacialExpressionDetectionModel(num_classes, num_heads).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ab8a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FER_model.txt', 'w') as f:\n",
    "    f.write(str(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a07a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for data, targets in dataloader:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Use autocast for mixed precision\n",
    "        with autocast():\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        # Scale the loss and perform backpropagation\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * data.size(0)\n",
    "        running_corrects += torch.sum(preds == targets.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31561c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in dataloader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            # Use autocast for mixed precision\n",
    "            with autocast():\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            running_loss += loss.item() * data.size(0)\n",
    "            running_corrects += torch.sum(preds == targets.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "488203d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  yolov5.models.common.Conv               [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  yolov5.models.common.Conv               [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  yolov5.models.common.C3                 [64, 64, 1]                   \n",
      "  3                -1  1     73984  yolov5.models.common.Conv               [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  yolov5.models.common.C3                 [128, 128, 2]                 \n",
      "  5                -1  1    295424  yolov5.models.common.Conv               [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  yolov5.models.common.C3                 [256, 256, 3]                 \n",
      "  7                -1  1   1180672  yolov5.models.common.Conv               [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  yolov5.models.common.C3                 [512, 512, 1]                 \n",
      "  9                -1  1    656896  yolov5.models.common.SPPF               [512, 512, 5]                 \n",
      " 10                -1  1    131584  yolov5.models.common.Conv               [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  yolov5.models.common.Concat             [1]                           \n",
      " 13                -1  1    361984  yolov5.models.common.C3                 [512, 256, 1, False]          \n",
      " 14                -1  1     33024  yolov5.models.common.Conv               [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  yolov5.models.common.Concat             [1]                           \n",
      " 17                -1  1     90880  yolov5.models.common.C3                 [256, 128, 1, False]          \n",
      " 18                -1  1    147712  yolov5.models.common.Conv               [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  yolov5.models.common.Concat             [1]                           \n",
      " 20                -1  1    296448  yolov5.models.common.C3                 [256, 256, 1, False]          \n",
      " 21                -1  1    590336  yolov5.models.common.Conv               [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  yolov5.models.common.Concat             [1]                           \n",
      " 23                -1  1   1182720  yolov5.models.common.C3                 [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  yolov5.models.yolo.Detect               [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "model = FacialExpressionDetectionModel(num_classes, num_heads).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, threshold=0.0001)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "828ea399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57c79dfa-79fd-493e-8382-5ab4c8c7de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d74a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200, Train Loss: 1.7807, Train Acc: 0.3845, Val Loss: 1.6926, Val Acc: 0.4720\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create empty lists to store loss and accuracy for each epoch\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate_epoch(model, val_dataloader, criterion, device)\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # append the loss and accuracy to the lists\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_acc)\n",
    "\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d739b73-3926-4de7-94eb-ec7cd6d2bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "dummy_input = torch.randn(1, 3, 128, 128).to(device)\n",
    "\n",
    "onnx_filename = \"FER_Model_Adam.onnx\"\n",
    "torch.onnx.export(model, dummy_input, onnx_filename)\n",
    "\n",
    "print(f\"Model saved as {onnx_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c3ba05-e6dc-4393-877b-a795bc38c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_acc_list = [float(t) for t in train_acc_list]\n",
    "val_acc_list = [float(t) for t in val_acc_list]\n",
    "\n",
    "train_acc_df = pd.DataFrame({'Epoch': range(len(train_acc_list)), 'Accuracy': train_acc_list, 'Type': 'Train'})\n",
    "val_acc_df = pd.DataFrame({'Epoch': range(len(val_acc_list)), 'Accuracy': val_acc_list, 'Type': 'Validation'})\n",
    "\n",
    "acc_df = pd.concat([train_acc_df, val_acc_df])\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=acc_df, x='Epoch', y='Accuracy', hue='Type')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Save the graph to a file\n",
    "plt.savefig('accuracy_graph_Adam.png', dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d842c20a-c235-46aa-944d-8f5be5a6419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = [float(t) for t in train_loss_list]\n",
    "val_loss_list = [float(t) for t in val_loss_list]\n",
    "\n",
    "train_loss_list = pd.DataFrame({'Epoch': range(len(train_loss_list)), 'Loss': train_loss_list, 'Type': 'Train'})\n",
    "val_loss_list = pd.DataFrame({'Epoch': range(len(val_loss_list)), 'Loss': val_loss_list, 'Type': 'Validation'})\n",
    "\n",
    "acc_df = pd.concat([train_loss_list, val_loss_list])\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=acc_df, x='Epoch', y='Loss', hue='Type')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Save the graph to a file\n",
    "plt.savefig('loss_graph_Adam.png', dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db09c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a function to check pytorch version\n",
    "def check_pytorch_version():\n",
    "    if torch.__version__ >= '1.6.0':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# define a function to check cuda version\n",
    "def check_cuda_version():\n",
    "    if torch.cuda.is_available():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# define a function to check cudnn version\n",
    "def check_cudnn_version():\n",
    "    if check_cuda_version():\n",
    "        if torch.backends.cudnn.enabled:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# define a function to check if the system is ready for training\n",
    "def check_system():\n",
    "    if check_pytorch_version():\n",
    "        print('PyTorch version: {}'.format(torch.__version__))\n",
    "    else:\n",
    "        print('PyTorch version: {} (update required)'.format(torch.__version__))\n",
    "        \n",
    "    if check_cuda_version():\n",
    "        print('CUDA version: {}'.format(torch.version.cuda))\n",
    "    else:\n",
    "        print('CUDA version: {} (install CUDA to enable GPU training)'.format(torch.version.cuda))\n",
    "        \n",
    "    if check_cudnn_version():\n",
    "        print('cuDNN version: {}'.format(torch.backends.cudnn.version()))\n",
    "    else:\n",
    "        print('cuDNN version: {} (install cuDNN to enable GPU training)'.format(torch.backends.cudnn.version()))\n",
    "\n",
    "check_system()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
